\documentclass{article}
% Extra packages for graphics, header control, good math typesetting, and margin
% control:
\usepackage{graphicx, fancyhdr, amssymb, amsmath, geometry}
\geometry{ left = 1.25in, right = 1.25in, top = 1.25in, bottom = 1.25in }
\pagestyle{fancy}
\setlength{\headsep}{.5in}
\renewcommand{\headrulewidth}{.25pt}
\lhead{\footnotesize BLG 454E Learning From Data}
\chead{}
\rhead{\footnotesize \today}

\lfoot{}
\cfoot{\footnotesize Page \thepage}
\rfoot{}
% The right way to define math operators, so that they display with the correct
% font and spacing:
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\CondProb}{Pr}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\moves}{moves}
\DeclareMathOperator{\without}{without}
\DeclareMathOperator{\visiting}{visiting}
\DeclareMathOperator{\pointG}{G}

\fancyhead[C]{\em Term Project Report}

    \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
    \def\independenT#1#2{\mathrel{\setbox0\hbox{$#1#2$}%
    \copy0\kern-\wd0\mkern4mu\box0}} 

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}
\newcommand{\writex}[2]{{\it{#1\textsubscript#2}}}
\newcommand{\writey}[1]{{\it{#1}}}
\newcommand{\Prob}{Pr}

\begin{document}
\title{\bf BLG 454E Learning From Data (Spring 2018)}
\author{\bf Homework I}
\date{}
\maketitle


\section{Question 1}
Weekend rain probability relationships:
\begin{equation*}
\begin{aligned}
	X : Saturday \\
	Y : Sunday \\
	\CondProb{(X = 1)} = 0.25 \\
	\CondProb{(X = 0)} = 0.75 \\
	\CondProb{(Y = 1 \mid X = 1)} = 0.50 \\
	\CondProb{(Y = 1 \mid X = 0)} = 0.25 
\end{aligned}
\end{equation*}
	We can obtain the probability that it rained on Saturday given that it rained on Sunday by using the probabilities given above and Bayes' Theorem:
\begin{equation*}
\begin{split}
	\CondProb{(X = 1 \mid Y = 1)} &=\frac{\CondProb{(Y = 1 \mid X = 1)}\times \CondProb{(X = 1)}}{\CondProb{(Y = 1 \mid X = 1)}\times \CondProb{(X = 1)} + \CondProb{(Y = 1 \mid X = 0)}\times \CondProb{(X = 0)}} \\
	&= \frac{0.50\times 0.25}{0.50\times 0.25 + 0.25\times 0.75} \\
	&= \frac{0.125}{0.3125} \\
	&= \boxed{0.4}
\end{split}
\end{equation*}
\section{Question 2}
Since we are asked to find probability that the bug reaches point A in 2 moves or less and starting at A will reach the point in 0 moves, there are 3 cases to be examined: reaching the point in 0 moves, in 1 moves and in 2 moves.

The probability for the case where reaching the point in 0 moves:  
\begin{equation*}
	\begin{aligned}[c]
	P: Point \\
	M: Move \\
	\CondProb{(P = A)} = \frac{1}{7} \\
	\CondProb{(0 \moves)} = \frac{1}{7}
	\end{aligned}
\end{equation*}
 
The probability of selecting the starting point which may lead to reach in 1 moves:
\begin{equation*}
	\begin{aligned}[c]
	\CondProb{(P = B)} = \CondProb{(P = G)} = \CondProb{(P = F)} = \frac{1}{7} \\
	\end{aligned}
\end{equation*}
The probability of reaching to point A from the selected point:
\begin{equation*}
	\begin{aligned}[c]
	\CondProb{(M = A \mid P = B)} = \frac{1}{3} \\
	\CondProb{(M = A \mid P = G)} = \frac{1}{6} \\
	\CondProb{(M = A \mid P = F)} = \frac{1}{3} \\
	\end{aligned}
\end{equation*}
The probability of reaching to point A from the starting points B,G, and F:
\begin{equation*}
	\begin{aligned}[c]
	\CondProb{(P = B \mid M = A)} = \CondProb{(P = B)}\times\CondProb{(M = A \mid P = B)} = \frac{1}{7}\times\frac{1}{3}  = \frac{1}{21}  \\
	\CondProb{(P = G \mid M = A)} = \CondProb{(P = G)}\times\CondProb{(M = A \mid P = G)} = \frac{1}{7}\times\frac{1}{6}  = \frac{1}{42}  \\
	\CondProb{(P = F \mid M = A)} = \CondProb{(P = F)}\times\CondProb{(M = A \mid P = F)} = \frac{1}{7}\times\frac{1}{3}  = \frac{1}{21}  \\
	\end{aligned}
\end{equation*}
The probability for the case where reaching the point in 1 moves:
\begin{equation*}
	\begin{aligned}[c]	
	\CondProb{(1 \moves)} =  \frac{1}{21} +  \frac{1}{42} + \frac{1}{21} =  \frac{5}{42}
	\end{aligned}
\end{equation*}

The probability of reaching to point G from points x = \{A, B, C, D, E and F\} similar to the previous case:
\begin{equation*}
	\begin{aligned}[c]
	\CondProb{(P = x \mid M = G)} = \CondProb{(P = x)}\times\CondProb{(M = A \mid P = x)} = \frac{1}{7}\times\frac{1}{3}  = \frac{1}{21}  \\
	\end{aligned}
\end{equation*}

The probability of reaching to point A from point G given that bug reaches to point G from x = \{A, B, C, D, E and F\}:
\begin{equation}
	\begin{aligned}[c]
	\CondProb{((P = G \mid S = x) \mid M = A))} = \CondProb{(P = x \mid M = G)}\times\CondProb{(M = A \mid P = G)} = \frac{1}{21}\times\frac{1}{6}  = \frac{1}{126}  \\
	\end{aligned}
\end{equation}

The probability of reaching to point A in 2 moves by visiting point G can be calculated by summing up the probabilities found for starting point A, B, C, D, E and F in equation 1:
\begin{equation*}
	\begin{aligned}[c]
	\CondProb{(2 \moves \visiting \pointG)} = 6\times\CondProb{((P = G \mid S = x) \mid M = A))} = 6\times\frac{1}{126} =  \frac{1}{21} \\
	\end{aligned}
\end{equation*}

The probability of reaching to point A in 2 moves without visiting point G from y = \{A, C, E\} using x = \{B, F\}:
\begin{equation}
	\begin{aligned}[c]
	\CondProb{((P = x \mid M = y) \mid M = A)} = \CondProb{(P = y)}\times\CondProb{(M = x \mid P = y)}\times\CondProb{(M = A \mid P = x)} = \frac{1}{7}\times\frac{1}{3}\times\frac{1}{3} = \frac{1}{63}  \\
	\end{aligned}
\end{equation}

Note that the bug can reach to either B or F and can reach to point A if it selects starting point as A. The probability of reaching to point A in 2 moves without visiting point G can be calculated by summing up the probabilities found for starting point A, C, and E in equation 2:
\begin{equation*}
	\begin{aligned}[c]
	\CondProb{(2 \moves \without \visiting \pointG)} = 4\times\CondProb{((P = G \mid S = x) \mid M = A))} = 4\times\frac{1}{63} =  \frac{4}{63} \\
	\end{aligned}
\end{equation*}

By choosing G as starting point and visiting points x = \{B, F\} the bug can reach to A in 2 moves with probability:
\begin{equation*}
	\begin{aligned}[c]
	\CondProb{((P = x \mid M = G) \mid M = A)} = \CondProb{(P = G)}\times\CondProb{(M = x \mid P = G)}\times\CondProb{(M = A \mid P = x)} = \frac{1}{7}\times\frac{1}{6}\times\frac{1}{3} = \frac{1}{126}  \\
	\CondProb{(S = G)} = 2\times\frac{1}{126} = \frac{1}{63}
	\end{aligned}
\end{equation*}

Hence, the probability for the case where reaching the point in 2 moves:
\begin{equation*}
	\begin{aligned}[c]	
	\CondProb{(2 \moves)} =  \CondProb{(2 \moves \visiting \pointG)} + \CondProb{(2 \moves \without \visiting \pointG)} + \CondProb{(S = G)} = \frac{1}{21} +  \frac{4}{63} + \frac{1}{63}=  \frac{8}{63}
	\end{aligned}
\end{equation*}

Therefore, the probability the probability that the bug reaches point A in 2 moves or less as donated by event A:
\begin{equation*}
	\begin{aligned}[c]	
	\CondProb{(A)} =  \CondProb{(0 \moves)} + \CondProb{(1\moves)}  + \CondProb{(2\moves)}= \frac{1}{7} +  \frac{5}{42} +  \frac{8}{63} = \boxed{\frac{7}{18}}
	\end{aligned}
\end{equation*}

\section{Question 3}
A
\section{Question 4}
\subsection{}
The prior probability for \writey{y}:
\begin{equation*}
	\begin{aligned}[c]
	\CondProb{(y = -)} = \frac{5}{10}\\
	\end{aligned}
	\quad\quad\quad
	\begin{aligned}[c]
	\CondProb{(y = +)} = \frac{5}{10} \\
	\end{aligned}
\end{equation*}
The likelihood probabilities of feature vectors \writex{x}{1}, \writex{x}{2}, \writex{x}{3}:
\begin{equation*}
	\begin{aligned}[c]
	\CondProb{(x_1 = 0 \mid y = -)} = \frac{3}{5} \\
	\CondProb{(x_1 = 1 \mid y = -)} = \frac{2}{5} \\ 
	\CondProb{(x_2 = 0 \mid y = -)} = \frac{3}{5} \\
	\CondProb{(x_2 = 1 \mid y = -)} = \frac{2}{5} \\ 
	\CondProb{(x_3 = 0 \mid y = -)} = \frac{4}{5} \\
	\CondProb{(x_3 = 1 \mid y = -)} = \frac{1}{5}
	\end{aligned}
	\quad\quad\quad
	\begin{aligned}[c]
	\CondProb{(x_1 = 0 \mid y = +)} = \frac{2}{5} \\
	\CondProb{(x_1 = 1 \mid y = +)} = \frac{3}{5} \\ 
	\CondProb{(x_2 = 0 \mid y = +)} = \frac{3}{5} \\
	\CondProb{(x_2 = 1 \mid y = +)} = \frac{2}{5} \\ 
	\CondProb{(x_3 = 0 \mid y = +)} = \frac{1}{5} \\
	\CondProb{(x_3 = 1 \mid y = +)} = \frac{4}{5} 
\end{aligned}
\end{equation*}
The Naive Bayes classifier for the given dataset:
\begin{equation}
y = \argmax_y\CondProb{(y)} \prod_{i=1}^{3} \CondProb{(x_i  \mid y)}
\end{equation}

The posterior probabilities of feature vectors \writex{x}{1}, \writex{x}{2}, \writex{x}{3} being \writey{y} = - which is multiplication of the likelihood probability of that vector and the class prior probability of \writey{y} = - if we omit the probability of the data \Prob(\writex{x}{i}) since it is same for all classes:
\begin{equation*}
	\begin{aligned}
	\CondProb{(y = -  \mid x_1 = 0, x_2 = 0, x_3 = 0)} = \frac{3}{5}\times\frac{3}{5}\times\frac{4}{5}\times\frac{5}{10} =  0.144 \\
	\CondProb{(y = -  \mid x_1 = 0, x_2 = 0, x_3 = 1)} = \frac{3}{5}\times\frac{3}{5}\times\frac{1}{5}\times\frac{5}{10} = 0.036 \\
	\CondProb{(y = -  \mid x_1 = 0, x_2 = 1, x_3 = 0)} = \frac{3}{5}\times\frac{2}{5}\times\frac{4}{5}\times\frac{5}{10}  = 0.096\\
	\CondProb{(y = -  \mid x_1 = 0, x_2 = 1, x_3 = 1)} = \frac{3}{5}\times\frac{2}{5}\times\frac{1}{5}\times\frac{5}{10}  = 0.024 \\
	\CondProb{(y = -  \mid x_1 = 1, x_2 = 0, x_3 = 0)} = \frac{2}{5}\times\frac{3}{5}\times\frac{4}{5}\times\frac{5}{10} = 0.096 \\
	\CondProb{(y = -  \mid x_1 = 1, x_2 = 0, x_3 = 1)} = \frac{2}{5}\times\frac{3}{5}\times\frac{1}{5}\times\frac{5}{10} = 0.024 \\
	\CondProb{(y = -  \mid x_1 = 1, x_2 = 1, x_3 = 0)} = \frac{2}{5}\times\frac{2}{5}\times\frac{4}{5}\times\frac{5}{10}  = 0.065 \\
	\CondProb{(y = -  \mid x_1 = 1, x_2 = 1, x_3 = 1)} = \frac{2}{5}\times\frac{2}{5}\times\frac{1}{5}\times\frac{5}{10} = 0.016 \\
	\end{aligned}
\end{equation*}

If we do the same operations for  \writey{y} = + to calculate the posterior probabilities of feature vectors \writex{x}{1}, \writex{x}{2}, \writex{x}{3}:
\begin{equation*}
	\begin{aligned}
	\CondProb{(y = +  \mid x_1 = 0, x_2 = 0, x_3 = 0)} = \frac{2}{5}\times\frac{3}{5}\times\frac{1}{5}\times\frac{5}{10} =  0.024 \\
	\CondProb{(y = +  \mid x_1 = 0, x_2 = 0, x_3 = 1)} = \frac{2}{5}\times\frac{3}{5}\times\frac{4}{5}\times\frac{5}{10} =  0.096 \\
	\CondProb{(y = +  \mid x_1 = 0, x_2 = 1, x_3 = 0)} = \frac{2}{5}\times\frac{2}{5}\times\frac{1}{5}\times\frac{5}{10} =  0.016 \\
	\CondProb{(y = +  \mid x_1 = 0, x_2 = 1, x_3 = 1)} = \frac{2}{5}\times\frac{2}{5}\times\frac{4}{5}\times\frac{5}{10} =  0.064 \\
	\CondProb{(y = +  \mid x_1 = 1, x_2 = 0, x_3 = 0)} = \frac{3}{5}\times\frac{3}{5}\times\frac{1}{5}\times\frac{5}{10}  = 0.036 \\
	\CondProb{(y = +  \mid x_1 = 1, x_2 = 0, x_3 = 1)} = \frac{3}{5}\times\frac{3}{5}\times\frac{4}{5}\times\frac{5}{10}  = 0.096 \\
	\CondProb{(y = +  \mid x_1 = 1, x_2 = 1, x_3 = 0)} = \frac{3}{5}\times\frac{2}{5}\times\frac{1}{5}\times\frac{5}{10} =  0.024 \\
	\CondProb{(y = +  \mid x_1 = 1, x_2 = 1, x_3 = 1)} = \frac{3}{5}\times\frac{2}{5}\times\frac{4}{5}\times\frac{5}{10} = 0.096 \\
	\end{aligned}
\end{equation*}

\subsection{}
We can predict the class label for (\writex{x}{1}, \writex{x}{2}, \writex{x}{3}) data using trained Naive Bayes approach in part 4.a and Naive Bayes theorem, equation (1) in part 4.a, as follows:
\begin{equation*}
	\CondProb{(y = +  \mid x_1 = 1, x_2 = 1, x_3 = 1)} = 0.096\quad>\quad\CondProb{(y = -  \mid x_1 = 1, x_2 = 1, x_3 = 1)} = 0.016
\end{equation*}

Therefore, the class would be \writey{y} = + according to Naive Bayes classifier.

\subsection{}
The asked probability values to calculate: 
\begin{equation*}
	\begin{aligned}
	\CondProb{(x_1 = 1, x_2 = 1)} = \frac{2}{10}\\
	\CondProb{(x_1 = 1)} = \frac{5}{10}\\ 
	\CondProb{(x_2 = 1)} = \frac{5}{10}
	\end{aligned}
\end{equation*}

According to independence method of Probability theorem if two events are independent, then they should satisfy:
\begin{equation*}
	\begin{aligned}
	\CondProb{(x_1 = 1, x_2 = 1)} &= \CondProb{(x_1 = 1)}\times\CondProb{(x_2 = 1)} \\
	\frac{2}{10} &= \frac{5}{10}\times\frac{5}{10} = \frac{20}{100} \\
	\frac{2}{10} &= \frac{2}{10}
	\end{aligned}
\end{equation*}

It clearly shows that \writex{x}{1} and  \writex{x}{2} are independent. 

\end{document}
