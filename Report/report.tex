\documentclass{article}
% Extra packages for graphics, header control, good math typesetting, and margin
% control:
\usepackage{graphicx, fancyhdr, amssymb, amsmath, geometry}
\geometry{ left = 1.25in, right = 1.25in, top = 1.25in, bottom = 1.25in }
\pagestyle{fancy}
\setlength{\headsep}{.5in}
\renewcommand{\headrulewidth}{.25pt}
\lhead{\footnotesize BLG 454E Learning From Data}
\chead{}
\rhead{\footnotesize \today}

\lfoot{}
\cfoot{\footnotesize Page \thepage}
\rfoot{}
% The right way to define math operators, so that they display with the correct
% font and spacing:
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\CondProb}{Pr}
\DeclareMathOperator{\argmax}{argmax}
\fancyhead[C]{\em Term Project Report}

    \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
    \def\independenT#1#2{\mathrel{\setbox0\hbox{$#1#2$}%
    \copy0\kern-\wd0\mkern4mu\box0}} 

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}
\newcommand{\writex}[2]{{\it{#1\textsubscript#2}}}
\newcommand{\writey}[1]{{\it{#1}}}
\newcommand{\Prob}{Pr}

\begin{document}
\title{\bf BLG 454E Learning From Data (Spring 2018)}
\author{\bf Homework I}
\date{}
\maketitle


\section{Question 1}
Weekend rain probability relationships:
\begin{equation*}
\begin{aligned}
	X : Saturday \\
	Y : Sunday \\
	\CondProb{(X = 1)} = 0.25 \\
	\CondProb{(X = 0)} = 0.75 \\
	\CondProb{(Y = 1 \mid X = 1)} = 0.50 \\
	\CondProb{(Y = 1 \mid X = 0)} = 0.25 
\end{aligned}
\end{equation*}
	We can obtain the probability that it rained on Saturday given that it rained on Sunday by using the probabilities given above and Bayes' Theorem:
\begin{equation*}
\begin{split}
	\CondProb{(X = 1 \mid Y = 1)} &=\frac{\CondProb{(Y = 1 \mid X = 1)}\times \CondProb{(X = 1)}}{\CondProb{(Y = 1 \mid X = 1)}\times \CondProb{(X = 1)} + \CondProb{(Y = 1 \mid X = 0)}\times \CondProb{(X = 0)}} \\
	&= \frac{0.50\times 0.25}{0.50\times 0.25 + 0.25\times 0.75} \\
	&= \frac{0.125}{0.3125} \\
	&= \boxed{0.4}
\end{split}
\end{equation*}
\section{Question 2}
A
\section{Question 3}
A
\section{Question 4}
\subsection{}
The prior probability for \writey{y}:
\begin{equation*}
	\begin{aligned}[c]
	\CondProb{(y = -)} = \frac{5}{10}\\
	\end{aligned}
	\quad\quad\quad
	\begin{aligned}[c]
	\CondProb{(y = +)} = \frac{5}{10} \\
	\end{aligned}
\end{equation*}
The likelihood probabilities of feature vectors \writex{x}{1}, \writex{x}{2}, \writex{x}{3}:
\begin{equation*}
	\begin{aligned}[c]
	\CondProb{(x_1 = 0 \mid y = -)} = \frac{3}{5} \\
	\CondProb{(x_1 = 1 \mid y = -)} = \frac{2}{5} \\ 
	\CondProb{(x_2 = 0 \mid y = -)} = \frac{3}{5} \\
	\CondProb{(x_2 = 1 \mid y = -)} = \frac{2}{5} \\ 
	\CondProb{(x_3 = 0 \mid y = -)} = \frac{4}{5} \\
	\CondProb{(x_3 = 1 \mid y = -)} = \frac{1}{5}
	\end{aligned}
	\quad\quad\quad
	\begin{aligned}[c]
	\CondProb{(x_1 = 0 \mid y = +)} = \frac{2}{5} \\
	\CondProb{(x_1 = 1 \mid y = +)} = \frac{3}{5} \\ 
	\CondProb{(x_2 = 0 \mid y = +)} = \frac{3}{5} \\
	\CondProb{(x_2 = 1 \mid y = +)} = \frac{2}{5} \\ 
	\CondProb{(x_3 = 0 \mid y = +)} = \frac{1}{5} \\
	\CondProb{(x_3 = 1 \mid y = +)} = \frac{4}{5} 
\end{aligned}
\end{equation*}
The Naive Bayes classifier for the given dataset:
\begin{equation}
y = \argmax_y\CondProb{(y)} \prod_{i=1}^{3} \CondProb{(x_i  \mid y)}
\end{equation}

The posterior probabilities of feature vectors \writex{x}{1}, \writex{x}{2}, \writex{x}{3} being \writey{y} = - which is multiplication of the likelihood probability of that vector and the class prior probability of \writey{y} = - if we omit the probability of the data \Prob(\writex{x}{i}) since it is same for all classes:
\begin{equation*}
	\begin{aligned}
	\CondProb{(y = -  \mid x_1 = 0, x_2 = 0, x_3 = 0)} = \frac{3}{5}\times\frac{3}{5}\times\frac{4}{5}\times\frac{5}{10} =  0.144 \\
	\CondProb{(y = -  \mid x_1 = 0, x_2 = 0, x_3 = 1)} = \frac{3}{5}\times\frac{3}{5}\times\frac{1}{5}\times\frac{5}{10} = 0.036 \\
	\CondProb{(y = -  \mid x_1 = 0, x_2 = 1, x_3 = 0)} = \frac{3}{5}\times\frac{2}{5}\times\frac{4}{5}\times\frac{5}{10}  = 0.096\\
	\CondProb{(y = -  \mid x_1 = 0, x_2 = 1, x_3 = 1)} = \frac{3}{5}\times\frac{2}{5}\times\frac{1}{5}\times\frac{5}{10}  = 0.024 \\
	\CondProb{(y = -  \mid x_1 = 1, x_2 = 0, x_3 = 0)} = \frac{2}{5}\times\frac{3}{5}\times\frac{4}{5}\times\frac{5}{10} = 0.096 \\
	\CondProb{(y = -  \mid x_1 = 1, x_2 = 0, x_3 = 1)} = \frac{2}{5}\times\frac{3}{5}\times\frac{1}{5}\times\frac{5}{10} = 0.024 \\
	\CondProb{(y = -  \mid x_1 = 1, x_2 = 1, x_3 = 0)} = \frac{2}{5}\times\frac{2}{5}\times\frac{4}{5}\times\frac{5}{10}  = 0.065 \\
	\CondProb{(y = -  \mid x_1 = 1, x_2 = 1, x_3 = 1)} = \frac{2}{5}\times\frac{2}{5}\times\frac{1}{5}\times\frac{5}{10} = 0.016 \\
	\end{aligned}
\end{equation*}

If we do the same operations for  \writey{y} = + to calculate the posterior probabilities of feature vectors \writex{x}{1}, \writex{x}{2}, \writex{x}{3}:
\begin{equation*}
	\begin{aligned}
	\CondProb{(y = +  \mid x_1 = 0, x_2 = 0, x_3 = 0)} = \frac{2}{5}\times\frac{3}{5}\times\frac{1}{5}\times\frac{5}{10} =  0.024 \\
	\CondProb{(y = +  \mid x_1 = 0, x_2 = 0, x_3 = 1)} = \frac{2}{5}\times\frac{3}{5}\times\frac{4}{5}\times\frac{5}{10} =  0.096 \\
	\CondProb{(y = +  \mid x_1 = 0, x_2 = 1, x_3 = 0)} = \frac{2}{5}\times\frac{2}{5}\times\frac{1}{5}\times\frac{5}{10} =  0.016 \\
	\CondProb{(y = +  \mid x_1 = 0, x_2 = 1, x_3 = 1)} = \frac{2}{5}\times\frac{2}{5}\times\frac{4}{5}\times\frac{5}{10} =  0.064 \\
	\CondProb{(y = +  \mid x_1 = 1, x_2 = 0, x_3 = 0)} = \frac{3}{5}\times\frac{3}{5}\times\frac{1}{5}\times\frac{5}{10}  = 0.036 \\
	\CondProb{(y = +  \mid x_1 = 1, x_2 = 0, x_3 = 1)} = \frac{3}{5}\times\frac{3}{5}\times\frac{4}{5}\times\frac{5}{10}  = 0.096 \\
	\CondProb{(y = +  \mid x_1 = 1, x_2 = 1, x_3 = 0)} = \frac{3}{5}\times\frac{2}{5}\times\frac{1}{5}\times\frac{5}{10} =  0.024 \\
	\CondProb{(y = +  \mid x_1 = 1, x_2 = 1, x_3 = 1)} = \frac{3}{5}\times\frac{2}{5}\times\frac{4}{5}\times\frac{5}{10} = 0.096 \\
	\end{aligned}
\end{equation*}

\subsection{}
We can predict the class label for (\writex{x}{1}, \writex{x}{2}, \writex{x}{3}) data using trained Naive Bayes approach in part 4.a and Naive Bayes theorem, equation (1) in part 4.a, as follows:
\begin{equation*}
	\CondProb{(y = +  \mid x_1 = 1, x_2 = 1, x_3 = 1)} = 0.096\quad>\quad\CondProb{(y = -  \mid x_1 = 1, x_2 = 1, x_3 = 1)} = 0.016
\end{equation*}

Therefore, the class would be \writey{y} = + according to Naive Bayes classifier.

\subsection{}
The asked probability values to calculate: 
\begin{equation*}
	\begin{aligned}
	\CondProb{(x_1 = 1, x_2 = 1)} = \frac{2}{10}\\
	\CondProb{(x_1 = 1)} = \frac{5}{10}\\ 
	\CondProb{(x_2 = 1)} = \frac{5}{10}
	\end{aligned}
\end{equation*}

According to independence method of Probability theorem if two events are independent, then they should satisfy:
\begin{equation*}
	\begin{aligned}
	\CondProb{(x_1 = 1, x_2 = 1)} &= \CondProb{(x_1 = 1)}\times\CondProb{(x_2 = 1)} \\
	\frac{2}{10} &= \frac{5}{10}\times\frac{5}{10} = \frac{20}{100} \\
	\frac{2}{10} &= \frac{2}{10}
	\end{aligned}
\end{equation*}

It clearly shows that \writex{x}{1} and  \writex{x}{2} are independent. 

\end{document}
